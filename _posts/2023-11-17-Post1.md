---
layout: post
title: Run distributed pytorch on GCP's Batch Service
---

## Situation and the problem

We need to run a model-parallel step on GCP's Batch service with `torchrun`. The script worked pretty well in a multi-GPU linux 
developer machine, but it failed when being executed on GCP Batch service under the same machine configuration. The error log
looks very mysterious. 

```
  File "/opt/conda/lib/python3.10/site-packages/triton/compiler/compiler.py", line 425, in compile
    so_path = make_stub(name, signature, constants)
  File "/opt/conda/lib/python3.10/site-packages/triton/compiler/make_launcher.py", line 39, in make_stub
    so = _build(name, src_path, tmpdir)
  File "/opt/conda/lib/python3.10/site-packages/triton/common/build.py", line 61, in _build
    cuda_lib_dirs = libcuda_dirs()
  File "/opt/conda/lib/python3.10/site-packages/triton/common/build.py", line 30, in libcuda_dirs
    assert any(os.path.exists(os.path.join(path, 'libcuda.so')) for path in dirs), msg
AssertionError: libcuda.so cannot found!
```

This issue never happened before when we only uses a single GPU to launch pytorch scripts, as we believed that the driver
is properly mounted into the container, as instructed by [Google's official LLM example](https://github.com/GoogleCloudPlatform/llm-pipeline-examples/blob/589e22181afe216da4f464fe9d6f61b1edef564c/scripts/train/run_batch.sh#L65-L66).

```yaml
                        volumes:
                          - /var/lib/nvidia/lib64:/usr/local/nvidia/lib64
                          - /var/lib/nvidia/bin:/usr/local/nvidia/bin
```

*In addition, I don't exactly know why torch depends on `triton`. By reading their source code, it seems that it is related to how their
new [`TorchInductor` module](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747) 
manages distributed cudagraph.*

## Triton 

Triton 2.1.0 version uses `/sbin/ldconfig -p` to find linkable `libcuda.so` ([Source](https://github.com/openai/triton/blob/c4628df33e6de9680e105d3ceafe7ce9d0a31123/python/triton/common/build.py#L25-L26)). 
Unfortunately it cannot find the `libcuda.so` if your docker runtime is not `nvidia-container-runtime`.  

+ Default docker runtime output
    ```
    $ docker run -it pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime bash
    root@2d251c5d518b:/workspace# /sbin/ldconfig -p | grep cuda
    # nothing
    ```

+ `nvidia` docker runtime output
    ```
    $ docker run -it --runtime=nvidia pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime bash
    root@5b77c2521774:/workspace# /sbin/ldconfig -p | grep cuda
    libcudadebugger.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcudadebugger.so.1
    libcuda.so.1 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcuda.so.1
    ```

## Nvidia Container Toolkit

We know that `ldconfig` command leverages the `/etc/ld.so.conf.d/*.conf` file to determine its search directories. 
Even you have mounted driver/cuda files including the `libcuda.so` through `/var/lib/nvidia/lib64:/usr/local/nvidia/lib64`,
it does not help the `ldconfig` find them as a typical `ldconfig`'s configuration file is like the following:
```
# libc default configuration
/usr/local/lib
# Multiarch support
/usr/local/lib/x86_64-linux-gnu
/lib/x86_64-linux-gnu
/usr/lib/x86_64-linux-gnu
```
It does not include any mounted path `/usr/local/nvidia/lib64` as the google official example demonstrated, thus `ldconfig`
isn't capable of finding the `libcuda.so`.

In contrast, [Nvidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)
does the thing right. It automatically loads all necessary libraries into the container's correct paths to allow scripts 
running inside the container to access GPUs properly. 

Eventually, to execute `torchrun` script properly on GCP Batch service, we run two scripts on the host machine to configure
`nvidia` runtime first (GCP G2 machine has already installed the `nvidia-container-toolkit` but did not configure it for docker.);
then we execute the torchrun script nvidia-runtime options. 

The following is our example:

```yaml
                    - script:
                        text: "sudo nvidia-ctk runtime configure --runtime=docker"
                    - script:
                        text: "sudo systemctl restart docker"
                    - container:
                        # ... other fields specifying command arguments and imageUri 
                        entrypoint: torchrun
                        options: --privileged --shm-size=4gb --runtime=nvidia --gpus all
                        # `torchrun` usually requires a hugh shm size (default to be 64MB on docker). 
```




